{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Explained by StatQuest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video Link](https://www.youtube.com/watch?v=AsNTP8Kwu80&t=226s&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video explains Recurrent Neural Networks (RNNs) in a clear and understandable manner, using stock market data as an example. It highlights the importance of RNNs in handling sequential data and discusses the challenges like the Vanishing/Exploding Gradient Problem. The video concludes by mentioning Long Short-Term Memory Networks as a solution to these issues.\n",
    "\n",
    "Highlights:\n",
    "- ðŸ§  Recurrent Neural Networks (RNNs) are essential for processing sequential data like stock market prices.\n",
    "- ðŸ“‰ The Vanishing/Exploding Gradient Problem makes training RNNs challenging due to large or small gradient values.\n",
    "- ðŸ¤– Long Short-Term Memory Networks are a popular solution to the issues faced by basic RNNs.\n",
    "\n",
    "Keywords\n",
    "- RNNs\n",
    "- Neural Networks\n",
    "- Stock Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks are designed to handle sequential data like time series, speech, and text. Vanilla Neural Networks are not suitable for sequential data because they have fixed input and output sizes, but sequential data can have varying lengths. RNNs solve this problem by processing sequential data one element at a time, maintaining a hidden state that captures the context of the previous elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolled RNN Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNNs Structure](./img/RNN-structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One input was fed into the RNN at each time step. The RNN processed the input and produced an output and a hidden state. The Outputs usually are ignored until the last step. The hidden state was fed into the feedback loop, used as input for the next time step. This process was repeated for each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrolled RNN Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/2024-04-10-09-31-36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the sequence length is 3. The RNN is unrolled to show the structure at each time step. The input at each time step is fed into the RNN, which produces an output and a hidden state. The hidden state is used as input for the next time step. The weights and biases are shared across all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exploding Gradient](./img/2024-04-10-09-40-33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any value of $w_2$ larger than 1, the gradient will explode. If $w_2$ equals 2 and sequence length is 50, $input_1$ will times 2^50 at the end. The output will be a huge number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![exploding gradients](./img/2024-04-10-09-48-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tried to train this RNN with backpropagation, this HUGE NUMBER would find its way into some of the gradients. And that would make it hard to take small steps to find the optimal weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vanishing gradients](./img/2024-04-10-09-51-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set $w_2$ to 0.5, the gradient will vanish. The gradient will be 0.5^50 at the end. The output will be a very small number close to 0. Now, when optimizing the weights and biases, instead of taking steps that are too large, we end up taking steps that are too small.\n",
    "\n",
    "The solution to exploding/vanishing gradients is Long Short-Term Memory Networks (LSTMs). LSTMs have a more complex structure that allows them to remember long-term dependencies and avoid the vanishing/exploding gradient problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
